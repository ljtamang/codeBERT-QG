{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO+vQwfzw2Q3UtSJk9kNFd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljtamang/codeBERT-QG/blob/master/main-ACmt-v2-squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pacakages required for successfully running this project."
      ],
      "metadata": {
        "id": "IKKRzu4LYvvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install psutil\n",
        "# !pip install h5py \n",
        "# !pip install typing-extensions \n",
        "# !pip install wheel \n",
        "!pip install blue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv_8MJCYAgMW",
        "outputId": "064fbec9-a1e2-48f3-ec0f-195c4699cf54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting blue\n",
            "  Downloading blue-0.9.1-py3-none-any.whl (10 kB)\n",
            "Collecting black==22.1.0\n",
            "  Downloading black-22.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flake8<5.0.0,>=3.8\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.11.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black==22.1.0->blue) (4.4.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from black==22.1.0->blue) (2.0.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black==22.1.0->blue) (2.6.2)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mccabe, pyflakes, pycodestyle, pathspec, mypy-extensions, click, flake8, black, blue\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed black-22.1.0 blue-0.9.1 click-8.1.3 flake8-4.0.1 mccabe-0.6.1 mypy-extensions-1.0.0 pathspec-0.11.0 pycodestyle-2.8.0 pyflakes-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the source code and data used for this project from github repository"
      ],
      "metadata": {
        "id": "f6OLvA_lTB-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ljtamang/codeBERT-QG.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l694QXfL-Uxv",
        "outputId": "6da823a6-cfe4-469b-d599-192c037f2935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'codeBERT-QG'...\n",
            "remote: Enumerating objects: 374, done.\u001b[K\n",
            "remote: Counting objects: 100% (243/243), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 374 (delta 99), reused 171 (delta 56), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (374/374), 92.21 MiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Navigate inside the recently downloaded codeBERT-QG project folder"
      ],
      "metadata": {
        "id": "NUHSNiw5amwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd codeBERT-QG\n",
        "#%cd codeBERT-QG/code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyA5rzWq9zsO",
        "outputId": "b0082462-3280-42b9-ed23-792eb12d130c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/codeBERT-QG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run execute java_script.sh . This will take care of everything and the project should build the model, validate and test it too."
      ],
      "metadata": {
        "id": "Ed53PSxHT04D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#when GPU 1 is available\n",
        "!bash java_script.sh 0 CbQg-SqD-v2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKxlAA1JD9qi",
        "outputId": "c32c1d37-691a-4255-b762-60a9a24345a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 00:28:10.066293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 00:28:10.219609: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-15 00:28:10.952613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 00:28:10.952716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 00:28:10.952733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
            "Downloading (…)lve/main/config.json: 100% 498/498 [00:00<00:00, 78.6kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 9.01MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 4.91MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 150/150 [00:00<00:00, 66.3kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 11.4kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 499M/499M [00:13<00:00, 36.4MB/s]\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:idx: 0\n",
            "INFO:__main__:source_tokens: ['<s>', 'Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '</s>', 'Arch', 'itect', 'urally', ',', '_the', '_school', '_has', '_a', '_Catholic', '_character', '.', '_At', 'op', '_the', '_Main', '_Building', \"'s\", '_gold', '_dome', '_is', '_a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '.', '_Immediately', '_in', '_front', '_of', '_the', '_Main', '_Building', '_and', '_facing', '_it', ',', '_is', '_a', '_copper', '_statue', '_of', '_Christ', '_with', '_arms', '_up', 'raised', '_with', '_the', '_legend', '_\"', 'Ven', 'ite', '_Ad', '_Me', '_Om', 'nes', '\".', '_Next', '_to', '_the', '_Main', '_Building', '_is', '_the', '_Basil', 'ica', '_of', '_the', '_Sacred', '_Heart', '.', '_Immediately', '_behind', '_the', '_basil', 'ica', '_is', '_the', '_Gro', 'tto', ',', '_a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '.', '_It', '_is', '_a', '_replica', '_of', '_the', '_gro', 'tto', '_at', '_L', 'our', 'des', ',', '_France', '_where', '_the', '_Virgin', '_Mary', '_rep', 'uted', 'ly', '_appeared', '_to', '_Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '_in', '_18', '58', '.', '_At', '_the', '_end', '_of', '_the', '_main', '_drive', '_(', 'and', '_in', '_a', '_direct', '_line', '_that', '_connects', '_through', '_3', '_statues', '_and', '_the', '_Gold', '_Dome', '),', '_is', '_a', '_simple', ',', '_modern', '_stone', '_statue', '_of', '_Mary', '.', '</s>']\n",
            "INFO:__main__:source_ids: 0 41742 6552 625 3398 208 22895 853 1827 2 37848 37471 28108 6 5 334 34 10 4019 2048 4 497 1517 5 4326 6919 18 1637 31346 16 10 9030 9577 9 5 9880 2708 4 29261 11 760 9 5 4326 6919 8 2114 24 6 16 10 7621 9577 9 4845 19 3701 62 33161 19 5 7875 22 39043 1459 1614 1464 13292 4977 845 4130 7 5 4326 6919 16 5 26429 2426 9 5 25095 6924 4 29261 639 5 32394 2426 16 5 7461 26187 6 10 19035 317 9 9621 8 12456 4 85 16 10 24633 9 5 11491 26187 23 226 2126 10067 6 1470 147 5 9880 2708 2851 13735 352 1382 7 6130 6552 625 3398 208 22895 853 1827 11 504 4432 4 497 5 253 9 5 1049 1305 36 463 11 10 2228 516 14 15230 149 155 19638 8 5 2610 25336 238 16 10 2007 6 2297 7326 9577 9 2708 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:target_tokens: ['<s>', 'To', '_whom', '_did', '_the', '_Virgin', '_Mary', '_allegedly', '_appear', '_in', '_18', '58', '_in', '_L', 'our', 'des', '_France', '?', '</s>']\n",
            "INFO:__main__:target_ids: 0 3972 2661 222 5 9880 2708 2346 2082 11 504 4432 11 226 2126 10067 1470 116 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:idx: 1\n",
            "INFO:__main__:source_tokens: ['<s>', 'a', '_copper', '_statue', '_of', '_Christ', '</s>', 'Arch', 'itect', 'urally', ',', '_the', '_school', '_has', '_a', '_Catholic', '_character', '.', '_At', 'op', '_the', '_Main', '_Building', \"'s\", '_gold', '_dome', '_is', '_a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '.', '_Immediately', '_in', '_front', '_of', '_the', '_Main', '_Building', '_and', '_facing', '_it', ',', '_is', '_a', '_copper', '_statue', '_of', '_Christ', '_with', '_arms', '_up', 'raised', '_with', '_the', '_legend', '_\"', 'Ven', 'ite', '_Ad', '_Me', '_Om', 'nes', '\".', '_Next', '_to', '_the', '_Main', '_Building', '_is', '_the', '_Basil', 'ica', '_of', '_the', '_Sacred', '_Heart', '.', '_Immediately', '_behind', '_the', '_basil', 'ica', '_is', '_the', '_Gro', 'tto', ',', '_a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '.', '_It', '_is', '_a', '_replica', '_of', '_the', '_gro', 'tto', '_at', '_L', 'our', 'des', ',', '_France', '_where', '_the', '_Virgin', '_Mary', '_rep', 'uted', 'ly', '_appeared', '_to', '_Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '_in', '_18', '58', '.', '_At', '_the', '_end', '_of', '_the', '_main', '_drive', '_(', 'and', '_in', '_a', '_direct', '_line', '_that', '_connects', '_through', '_3', '_statues', '_and', '_the', '_Gold', '_Dome', '),', '_is', '_a', '_simple', ',', '_modern', '_stone', '_statue', '_of', '_Mary', '.', '</s>']\n",
            "INFO:__main__:source_ids: 0 102 7621 9577 9 4845 2 37848 37471 28108 6 5 334 34 10 4019 2048 4 497 1517 5 4326 6919 18 1637 31346 16 10 9030 9577 9 5 9880 2708 4 29261 11 760 9 5 4326 6919 8 2114 24 6 16 10 7621 9577 9 4845 19 3701 62 33161 19 5 7875 22 39043 1459 1614 1464 13292 4977 845 4130 7 5 4326 6919 16 5 26429 2426 9 5 25095 6924 4 29261 639 5 32394 2426 16 5 7461 26187 6 10 19035 317 9 9621 8 12456 4 85 16 10 24633 9 5 11491 26187 23 226 2126 10067 6 1470 147 5 9880 2708 2851 13735 352 1382 7 6130 6552 625 3398 208 22895 853 1827 11 504 4432 4 497 5 253 9 5 1049 1305 36 463 11 10 2228 516 14 15230 149 155 19638 8 5 2610 25336 238 16 10 2007 6 2297 7326 9577 9 2708 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:target_tokens: ['<s>', 'What', '_is', '_in', '_front', '_of', '_the', '_Notre', '_Dame', '_Main', '_Building', '?', '</s>']\n",
            "INFO:__main__:target_ids: 0 2264 16 11 760 9 5 10579 9038 4326 6919 116 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:idx: 2\n",
            "INFO:__main__:source_tokens: ['<s>', 'the', '_Main', '_Building', '</s>', 'Arch', 'itect', 'urally', ',', '_the', '_school', '_has', '_a', '_Catholic', '_character', '.', '_At', 'op', '_the', '_Main', '_Building', \"'s\", '_gold', '_dome', '_is', '_a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '.', '_Immediately', '_in', '_front', '_of', '_the', '_Main', '_Building', '_and', '_facing', '_it', ',', '_is', '_a', '_copper', '_statue', '_of', '_Christ', '_with', '_arms', '_up', 'raised', '_with', '_the', '_legend', '_\"', 'Ven', 'ite', '_Ad', '_Me', '_Om', 'nes', '\".', '_Next', '_to', '_the', '_Main', '_Building', '_is', '_the', '_Basil', 'ica', '_of', '_the', '_Sacred', '_Heart', '.', '_Immediately', '_behind', '_the', '_basil', 'ica', '_is', '_the', '_Gro', 'tto', ',', '_a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '.', '_It', '_is', '_a', '_replica', '_of', '_the', '_gro', 'tto', '_at', '_L', 'our', 'des', ',', '_France', '_where', '_the', '_Virgin', '_Mary', '_rep', 'uted', 'ly', '_appeared', '_to', '_Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '_in', '_18', '58', '.', '_At', '_the', '_end', '_of', '_the', '_main', '_drive', '_(', 'and', '_in', '_a', '_direct', '_line', '_that', '_connects', '_through', '_3', '_statues', '_and', '_the', '_Gold', '_Dome', '),', '_is', '_a', '_simple', ',', '_modern', '_stone', '_statue', '_of', '_Mary', '.', '</s>']\n",
            "INFO:__main__:source_ids: 0 627 4326 6919 2 37848 37471 28108 6 5 334 34 10 4019 2048 4 497 1517 5 4326 6919 18 1637 31346 16 10 9030 9577 9 5 9880 2708 4 29261 11 760 9 5 4326 6919 8 2114 24 6 16 10 7621 9577 9 4845 19 3701 62 33161 19 5 7875 22 39043 1459 1614 1464 13292 4977 845 4130 7 5 4326 6919 16 5 26429 2426 9 5 25095 6924 4 29261 639 5 32394 2426 16 5 7461 26187 6 10 19035 317 9 9621 8 12456 4 85 16 10 24633 9 5 11491 26187 23 226 2126 10067 6 1470 147 5 9880 2708 2851 13735 352 1382 7 6130 6552 625 3398 208 22895 853 1827 11 504 4432 4 497 5 253 9 5 1049 1305 36 463 11 10 2228 516 14 15230 149 155 19638 8 5 2610 25336 238 16 10 2007 6 2297 7326 9577 9 2708 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:target_tokens: ['<s>', 'The', '_Basil', 'ica', '_of', '_the', '_Sacred', '_heart', '_at', '_Notre', '_Dame', '_is', '_beside', '_to', '_which', '_structure', '?', '</s>']\n",
            "INFO:__main__:target_ids: 0 133 26429 2426 9 5 25095 1144 23 10579 9038 16 13276 7 61 3184 116 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:idx: 3\n",
            "INFO:__main__:source_tokens: ['<s>', 'a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '</s>', 'Arch', 'itect', 'urally', ',', '_the', '_school', '_has', '_a', '_Catholic', '_character', '.', '_At', 'op', '_the', '_Main', '_Building', \"'s\", '_gold', '_dome', '_is', '_a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '.', '_Immediately', '_in', '_front', '_of', '_the', '_Main', '_Building', '_and', '_facing', '_it', ',', '_is', '_a', '_copper', '_statue', '_of', '_Christ', '_with', '_arms', '_up', 'raised', '_with', '_the', '_legend', '_\"', 'Ven', 'ite', '_Ad', '_Me', '_Om', 'nes', '\".', '_Next', '_to', '_the', '_Main', '_Building', '_is', '_the', '_Basil', 'ica', '_of', '_the', '_Sacred', '_Heart', '.', '_Immediately', '_behind', '_the', '_basil', 'ica', '_is', '_the', '_Gro', 'tto', ',', '_a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '.', '_It', '_is', '_a', '_replica', '_of', '_the', '_gro', 'tto', '_at', '_L', 'our', 'des', ',', '_France', '_where', '_the', '_Virgin', '_Mary', '_rep', 'uted', 'ly', '_appeared', '_to', '_Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '_in', '_18', '58', '.', '_At', '_the', '_end', '_of', '_the', '_main', '_drive', '_(', 'and', '_in', '_a', '_direct', '_line', '_that', '_connects', '_through', '_3', '_statues', '_and', '_the', '_Gold', '_Dome', '),', '_is', '_a', '_simple', ',', '_modern', '_stone', '_statue', '_of', '_Mary', '.', '</s>']\n",
            "INFO:__main__:source_ids: 0 102 19035 317 9 9621 8 12456 2 37848 37471 28108 6 5 334 34 10 4019 2048 4 497 1517 5 4326 6919 18 1637 31346 16 10 9030 9577 9 5 9880 2708 4 29261 11 760 9 5 4326 6919 8 2114 24 6 16 10 7621 9577 9 4845 19 3701 62 33161 19 5 7875 22 39043 1459 1614 1464 13292 4977 845 4130 7 5 4326 6919 16 5 26429 2426 9 5 25095 6924 4 29261 639 5 32394 2426 16 5 7461 26187 6 10 19035 317 9 9621 8 12456 4 85 16 10 24633 9 5 11491 26187 23 226 2126 10067 6 1470 147 5 9880 2708 2851 13735 352 1382 7 6130 6552 625 3398 208 22895 853 1827 11 504 4432 4 497 5 253 9 5 1049 1305 36 463 11 10 2228 516 14 15230 149 155 19638 8 5 2610 25336 238 16 10 2007 6 2297 7326 9577 9 2708 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:target_tokens: ['<s>', 'What', '_is', '_the', '_Gro', 'tto', '_at', '_Notre', '_Dame', '?', '</s>']\n",
            "INFO:__main__:target_ids: 0 2264 16 5 7461 26187 23 10579 9038 116 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:target_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:idx: 4\n",
            "INFO:__main__:source_tokens: ['<s>', 'a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '</s>', 'Arch', 'itect', 'urally', ',', '_the', '_school', '_has', '_a', '_Catholic', '_character', '.', '_At', 'op', '_the', '_Main', '_Building', \"'s\", '_gold', '_dome', '_is', '_a', '_golden', '_statue', '_of', '_the', '_Virgin', '_Mary', '.', '_Immediately', '_in', '_front', '_of', '_the', '_Main', '_Building', '_and', '_facing', '_it', ',', '_is', '_a', '_copper', '_statue', '_of', '_Christ', '_with', '_arms', '_up', 'raised', '_with', '_the', '_legend', '_\"', 'Ven', 'ite', '_Ad', '_Me', '_Om', 'nes', '\".', '_Next', '_to', '_the', '_Main', '_Building', '_is', '_the', '_Basil', 'ica', '_of', '_the', '_Sacred', '_Heart', '.', '_Immediately', '_behind', '_the', '_basil', 'ica', '_is', '_the', '_Gro', 'tto', ',', '_a', '_Marian', '_place', '_of', '_prayer', '_and', '_reflection', '.', '_It', '_is', '_a', '_replica', '_of', '_the', '_gro', 'tto', '_at', '_L', 'our', 'des', ',', '_France', '_where', '_the', '_Virgin', '_Mary', '_rep', 'uted', 'ly', '_appeared', '_to', '_Saint', '_Bern', 'ad', 'ette', '_S', 'oub', 'ir', 'ous', '_in', '_18', '58', '.', '_At', '_the', '_end', '_of', '_the', '_main', '_drive', '_(', 'and', '_in', '_a', '_direct', '_line', '_that', '_connects', '_through', '_3', '_statues', '_and', '_the', '_Gold', '_Dome', '),', '_is', '_a', '_simple', ',', '_modern', '_stone', '_statue', '_of', '_Mary', '.', '</s>']\n",
            "INFO:__main__:source_ids: 0 102 9030 9577 9 5 9880 2708 2 37848 37471 28108 6 5 334 34 10 4019 2048 4 497 1517 5 4326 6919 18 1637 31346 16 10 9030 9577 9 5 9880 2708 4 29261 11 760 9 5 4326 6919 8 2114 24 6 16 10 7621 9577 9 4845 19 3701 62 33161 19 5 7875 22 39043 1459 1614 1464 13292 4977 845 4130 7 5 4326 6919 16 5 26429 2426 9 5 25095 6924 4 29261 639 5 32394 2426 16 5 7461 26187 6 10 19035 317 9 9621 8 12456 4 85 16 10 24633 9 5 11491 26187 23 226 2126 10067 6 1470 147 5 9880 2708 2851 13735 352 1382 7 6130 6552 625 3398 208 22895 853 1827 11 504 4432 4 497 5 253 9 5 1049 1305 36 463 11 10 2228 516 14 15230 149 155 19638 8 5 2610 25336 238 16 10 2007 6 2297 7326 9577 9 2708 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:target_tokens: ['<s>', 'What', '_sits', '_on', '_top', '_of', '_the', '_Main', '_Building', '_at', '_Notre', '_Dame', '?', '</s>']\n",
            "INFO:__main__:target_ids: 0 2264 6476 15 299 9 5 4326 6919 23 10579 9038 116 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 83593\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  Num epoch = 20\n",
            "epoch 0 loss 5.6739: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 195.86305\n",
            "INFO:__main__:  global_step = 2614\n",
            "INFO:__main__:  train_loss = 5.6739\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best ppl:195.86305\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 12.82 | rouge_l = 18.49 | meteor = 10.32 | EM = 0.00 | Precision = 18.39 | Recall = 20.87 | F1 = 18.71 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:12.819846173344107\n",
            "INFO:__main__:  ********************\n",
            "epoch 1 loss 3.8186: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 52.5703\n",
            "INFO:__main__:  global_step = 5227\n",
            "INFO:__main__:  train_loss = 3.8186\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best ppl:52.5703\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 17.81 | rouge_l = 26.89 | meteor = 16.55 | EM = 0.00 | Precision = 31.86 | Recall = 27.92 | F1 = 28.57 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:17.80767490299041\n",
            "INFO:__main__:  ********************\n",
            "epoch 2 loss 2.8004: 100% 2613/2613 [28:18<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 25.17939\n",
            "INFO:__main__:  global_step = 7840\n",
            "INFO:__main__:  train_loss = 2.8004\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best ppl:25.17939\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 21.19 | rouge_l = 32.36 | meteor = 19.93 | EM = 0.00 | Precision = 41.01 | Recall = 31.43 | F1 = 34.29 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:21.187909369384407\n",
            "INFO:__main__:  ********************\n",
            "epoch 3 loss 2.275: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 20.94749\n",
            "INFO:__main__:  global_step = 10453\n",
            "INFO:__main__:  train_loss = 2.275\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best ppl:20.94749\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.68 | rouge_l = 34.37 | meteor = 22.24 | EM = 0.00 | Precision = 42.25 | Recall = 34.18 | F1 = 36.42 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:22.68204139382635\n",
            "INFO:__main__:  ********************\n",
            "epoch 4 loss 1.958: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 22.49339\n",
            "INFO:__main__:  global_step = 13066\n",
            "INFO:__main__:  train_loss = 1.958\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.86 | rouge_l = 34.44 | meteor = 22.36 | EM = 0.00 | Precision = 42.00 | Recall = 34.31 | F1 = 36.50 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:22.857928363015542\n",
            "INFO:__main__:  ********************\n",
            "epoch 5 loss 1.7139: 100% 2613/2613 [28:18<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 20.74489\n",
            "INFO:__main__:  global_step = 15679\n",
            "INFO:__main__:  train_loss = 1.7139\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best ppl:20.74489\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.78 | rouge_l = 35.44 | meteor = 22.82 | EM = 0.00 | Precision = 43.95 | Recall = 35.12 | F1 = 37.57 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:23.783406854405726\n",
            "INFO:__main__:  ********************\n",
            "epoch 6 loss 1.5128: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 22.44495\n",
            "INFO:__main__:  global_step = 18292\n",
            "INFO:__main__:  train_loss = 1.5128\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.11 | rouge_l = 33.98 | meteor = 22.55 | EM = 0.00 | Precision = 41.76 | Recall = 34.05 | F1 = 36.13 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 7 loss 1.3381: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 21.87434\n",
            "INFO:__main__:  global_step = 20905\n",
            "INFO:__main__:  train_loss = 1.3381\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.55 | rouge_l = 34.95 | meteor = 23.02 | EM = 0.00 | Precision = 43.64 | Recall = 34.31 | F1 = 37.03 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 8 loss 1.186: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 24.987\n",
            "INFO:__main__:  global_step = 23518\n",
            "INFO:__main__:  train_loss = 1.186\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.59 | rouge_l = 34.98 | meteor = 23.06 | EM = 0.00 | Precision = 42.89 | Recall = 35.15 | F1 = 37.14 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 9 loss 1.0489: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 25.89026\n",
            "INFO:__main__:  global_step = 26131\n",
            "INFO:__main__:  train_loss = 1.0489\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 24.48 | rouge_l = 36.57 | meteor = 24.12 | EM = 0.00 | Precision = 43.17 | Recall = 37.53 | F1 = 38.59 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:  Best bleu:24.475876363101268\n",
            "INFO:__main__:  ********************\n",
            "epoch 10 loss 0.9273: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 30.20472\n",
            "INFO:__main__:  global_step = 28744\n",
            "INFO:__main__:  train_loss = 0.9273\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.50 | rouge_l = 34.94 | meteor = 23.29 | EM = 0.00 | Precision = 42.41 | Recall = 35.43 | F1 = 37.14 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 11 loss 0.8185: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 32.03865\n",
            "INFO:__main__:  global_step = 31357\n",
            "INFO:__main__:  train_loss = 0.8185\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.01 | rouge_l = 33.89 | meteor = 22.86 | EM = 0.00 | Precision = 41.00 | Recall = 34.63 | F1 = 36.14 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 12 loss 0.7232: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 34.98966\n",
            "INFO:__main__:  global_step = 33970\n",
            "INFO:__main__:  train_loss = 0.7232\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.01 | rouge_l = 34.65 | meteor = 23.49 | EM = 0.00 | Precision = 41.45 | Recall = 36.09 | F1 = 37.11 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 13 loss 0.6399: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 35.12661\n",
            "INFO:__main__:  global_step = 36583\n",
            "INFO:__main__:  train_loss = 0.6399\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 23.36 | rouge_l = 34.68 | meteor = 23.43 | EM = 0.00 | Precision = 41.48 | Recall = 35.99 | F1 = 37.09 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 14 loss 0.5655: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 38.00867\n",
            "INFO:__main__:  global_step = 39196\n",
            "INFO:__main__:  train_loss = 0.5655\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.45 | rouge_l = 33.85 | meteor = 23.03 | EM = 0.00 | Precision = 40.12 | Recall = 35.39 | F1 = 36.16 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 15 loss 0.5042: 100% 2613/2613 [28:21<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 41.38088\n",
            "INFO:__main__:  global_step = 41809\n",
            "INFO:__main__:  train_loss = 0.5042\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.87 | rouge_l = 33.95 | meteor = 22.96 | EM = 0.00 | Precision = 40.88 | Recall = 35.13 | F1 = 36.36 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 16 loss 0.4532: 100% 2613/2613 [28:21<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 41.76213\n",
            "INFO:__main__:  global_step = 44422\n",
            "INFO:__main__:  train_loss = 0.4532\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.44 | rouge_l = 33.67 | meteor = 22.94 | EM = 0.00 | Precision = 39.96 | Recall = 35.03 | F1 = 35.91 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 17 loss 0.41: 100% 2613/2613 [28:19<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 44.00123\n",
            "INFO:__main__:  global_step = 47035\n",
            "INFO:__main__:  train_loss = 0.41\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.46 | rouge_l = 33.60 | meteor = 22.91 | EM = 0.00 | Precision = 39.81 | Recall = 35.47 | F1 = 36.02 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 18 loss 0.3767: 100% 2613/2613 [28:20<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 45.75852\n",
            "INFO:__main__:  global_step = 49648\n",
            "INFO:__main__:  train_loss = 0.3767\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.28 | rouge_l = 33.56 | meteor = 22.62 | EM = 0.00 | Precision = 39.91 | Recall = 35.09 | F1 = 35.87 | \n",
            "INFO:__main__:  ********************\n",
            "epoch 19 loss 0.3533: 100% 2613/2613 [28:21<00:00,  1.54it/s]\n",
            "INFO:__main__:\n",
            "***** Running evaluation *****\n",
            "INFO:__main__:  Num examples = 8314\n",
            "INFO:__main__:  Batch size = 32\n",
            "INFO:__main__:  eval_ppl = 45.49669\n",
            "INFO:__main__:  global_step = 52261\n",
            "INFO:__main__:  train_loss = 0.3533\n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:dev set: bleu = 22.38 | rouge_l = 33.66 | meteor = 23.07 | EM = 0.00 | Precision = 39.72 | Recall = 35.70 | F1 = 36.14 | \n",
            "INFO:__main__:  ********************\n",
            "INFO:__main__:\n",
            "***** Running Test *****\n",
            "INFO:__main__:Test file: ../squad-dataset/test/\n",
            "100% 260/260 [19:30<00:00,  4.50s/it]\n",
            "INFO:__main__:test set: bleu = 22.67 | rouge_l = 34.21 | meteor = 23.47 | EM = 0.00 | Precision = 40.84 | Recall = 36.16 | F1 = 36.86 | \n",
            "INFO:__main__:  ********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following shell if you want to load model that has been build and run test. You don't need to run it again becuase test has been carried out in above execution."
      ],
      "metadata": {
        "id": "fKHFKQE7UbhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Test\n",
        "#!bash java_script_test.sh 0 cbQG"
      ],
      "metadata": {
        "id": "ghuwcDAaOr6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive so you can copy the output to google drive. It is necessary becuase virtuan machine may be disconnected before you can copy."
      ],
      "metadata": {
        "id": "pb4nuWSdABkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WZ-mBZQOAMlP",
        "outputId": "12c597aa-6e1d-40d3-c419-a8bb70e58eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip the output so that you can download all files at once"
      ],
      "metadata": {
        "id": "B37CzUi2pqiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/file.zip /content/folder_to_zip\n",
        "!zip -r /content/CbQg-SqD-v2.zip /content/codeBERT-QG/output/CbQg-SqD-v2\n"
      ],
      "metadata": {
        "id": "0oYlpgK_pw6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b32ec6-5940-4082-fa40-88632bc11697"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/ (stored 0%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-last/ (stored 0%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-last/pytorch_model.bin (deflated 10%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-best-ppl/ (stored 0%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-best-ppl/pytorch_model.bin (deflated 10%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/dev.output (deflated 64%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/test_0.gold (deflated 72%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-best-bleu/ (stored 0%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/checkpoint-best-bleu/pytorch_model.bin (deflated 10%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/test_0.output (deflated 69%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/log.txt (deflated 89%)\n",
            "  adding: content/codeBERT-QG/output/CbQg-SqD-v2/dev.gold (deflated 66%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following code helps you to copy your output to your goole drive"
      ],
      "metadata": {
        "id": "306nwvv_qtg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# copy it there\n",
        "!cp /content/CbQg-SqD-v2.zip /content/drive/MyDrive/output"
      ],
      "metadata": {
        "id": "EmGUz8mwq04v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute following command if need inforamtion of system used for training."
      ],
      "metadata": {
        "id": "16oSLV0dUunh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'\n",
        "!nvidia-smi\n",
        "!nvidia-smi -L\n",
        "!lscpu |grep 'Model name'\n",
        "#no.of cores each processor is having \n",
        "!lscpu | grep 'Core(s) per socket:'\n",
        "#memory that we can use\n",
        "!free -h --si | awk  '/Mem:/{print $2}'\n",
        "#hard disk space that we can use\n",
        "!df -h / | awk '{print $4}'\n",
        "#no.of sockets i.e available slots for physical processors\n",
        "!lscpu | grep 'Socket(s):'\n",
        "!lscpu | grep \"L3 cache\" \n",
        "#if it had turbo boost it would've shown Min and Max MHz also but it is only showing current frequency this means it always operates at shown frequency\n",
        "!lscpu | grep \"MHz\""
      ],
      "metadata": {
        "id": "jloTmRB9Ka16",
        "outputId": "fd9d99d1-e673-4fa4-81a1-ebbadb93b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb 10 10:59:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    49W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-7f259069-ea49-f29a-5743-3cff0a147f5c)\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Core(s) per socket:              6\n",
            "85G\n",
            "Avail\n",
            "137G\n",
            "Socket(s):                       1\n",
            "L3 cache:                        38.5 MiB\n",
            "CPU MHz:                         2200.210\n"
          ]
        }
      ]
    }
  ]
}